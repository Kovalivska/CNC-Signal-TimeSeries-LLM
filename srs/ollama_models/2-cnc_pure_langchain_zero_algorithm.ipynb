{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Pure LangChain Zero-Algorithm Machine Data Analysis\n",
    "\n",
    "## Konzept: Vollst√§ndig algorithmfreier LangChain-Ansatz\n",
    "\n",
    "Dieses Notebook implementiert einen **reinen LangChain-Ansatz** ohne jede vordefinierte Algorithmen:\n",
    "- **LangChain** f√ºr strukturierte LLM-Integration\n",
    "- **Zero Algorithm Approach**: Keinerlei Vorannahmen √ºber Datenstruktur\n",
    "- **Universal Data Understanding**: Funktioniert mit beliebigen strukturierten Daten\n",
    "- **LLM-Generated Queries**: Model erstellt selbst√§ndig Datenabfragen\n",
    "\n",
    "### Kernprinzipien:\n",
    "1. **No Predefined Logic**: Keinerlei hartcodierte Gesch√§ftslogik\n",
    "2. **Universal Prompts**: Prompts funktionieren mit beliebigen Datenstrukturen\n",
    "3. **Self-Query Generation**: LLM generiert eigene Datenabfragen\n",
    "4. **Context-Aware Analysis**: LLM versteht Datenkontext autonom\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Was dieses Notebook umsetzt: Der Kern des \"Zero-Algorithm\"-Ansatzes\n",
    "\n",
    "Die zentrale Idee dieses Notebooks ist die Entwicklung eines Analysesystems, das **g√§nzlich ohne vordefinierte, im Code festgelegte Logik** auskommt. Anstatt dem Sprachmodell (LLM) explizite Anweisungen zu geben, wie z. B. \"Suche nach Zyklen in der Spalte `exec_STRING`\", wird das Modell aufgefordert, die Daten selbstst√§ndig zu untersuchen und eigenst√§ndig zu entscheiden, welche Informationen relevant sind.\n",
    "\n",
    "Dies wird durch einen **zweistufigen Prozess** erreicht, der mit dem Framework **LangChain** umgesetzt wird:\n",
    "\n",
    "1.  **Schritt 1: Autonomes Datenverst√§ndnis.** Zuerst werden dem LLM allgemeine Informationen √ºber die Daten (Spaltennamen, Datentypen, einige Beispielzeilen) pr√§sentiert. Die Aufgabe lautet: \"Analysiere diese Daten und beschreibe ihre Struktur, die wichtigsten Spalten und m√∂gliche Muster in einem JSON-Format.\" Das Modell kommt so von selbst zu dem Schluss, dass Spalten wie `exec_STRING` und `pgm_STRING` f√ºr die Analyse entscheidend sind.\n",
    "2.  **Schritt 2: Beantwortung der Frage unter Nutzung des Verst√§ndnisses.** Anschlie√üend wird die konkrete Frage des Benutzers an das Modell gesendet, zusammen mit der JSON-Zusammenfassung aus dem ersten Schritt. Der Befehl lautet nun sinngem√§√ü: \"Hier ist deine eigene Analyse dieser Daten. Beantworte auf dieser Grundlage meine Frage.\"\n",
    "\n",
    "Dieser Ansatz ist intelligenter, da das LLM nicht nur starren Anweisungen folgt, sondern seine **eigenen Schlussfolgerungen** als Kontext f√ºr die Antwort nutzt.\n",
    "\n",
    "---\n",
    "### Analyse des Codes in Schritten\n",
    "\n",
    "* **Schritte 1-2: Laden der Daten und LangChain-Client (`PureLangChainAnalyzer`)**\n",
    "    * Der Code l√§dt die Daten universell aus einer Excel-Datei.\n",
    "    * Die Klasse `PureLangChainAnalyzer` bildet das Kernst√ºck des Systems. Sie enth√§lt die Logik f√ºr den zweistufigen Prozess:\n",
    "        * `understand_data_universally` implementiert den ersten Schritt (das Verstehen).\n",
    "        * `answer_question_universally` implementiert den zweiten Schritt (die Beantwortung der Frage).\n",
    "    * Die Verwendung von **LangChain** (mit `PromptTemplate`, Verkettungen `|` und `StrOutputParser`) f√ºhrt zu einem saubereren, besser strukturierten und lesbareren Code im Vergleich zu den direkten HTTP-Anfragen in fr√ºheren Ans√§tzen.\n",
    "\n",
    "* **Schritt 3: Ausf√ºhrung des Datenverst√§ndnisses**\n",
    "    * Hier wird der erste Schritt des Prozesses ausgef√ºhrt. Das Notebook zeigt, dass das LLM die Daten analysiert und die Schl√ºsselspalten sowie deren potenzielle Bedeutung identifiziert hat. Dies beweist die F√§higkeit des Modells, Datenstrukturen autonom zu erlernen.\n",
    "\n",
    "* **Schritte 4-6: Beantwortung von Fragen und deren Validierung**\n",
    "    * Die Funktion `process_pure_langchain_question` startet den zweiten Schritt, indem eine spezifische Frage an das Modell gestellt wird.\n",
    "    * Zur √úberpr√ºfung der Genauigkeit wird dieselbe `ValidationAlgorithms`-Klasse wie zuvor verwendet. **Wichtig:** Diese Algorithmen dienen *ausschlie√ülich* der Bewertung und werden nicht zur Generierung der Antwort herangezogen. Dies stellt sicher, dass das \"Zero-Algorithm\"-Prinzip eingehalten wird.\n",
    "    * Die Klasse `PureLangChainAccuracyTester` vergleicht die Antwort des LLM mit der \"korrekten\" Antwort der Validierungsalgorithmen und berechnet daraus eine prozentuale Genauigkeit.\n",
    "\n",
    "---\n",
    "### üìä Analyse der Ergebnisse: Durchbruch und Grenzen\n",
    "\n",
    "Dies ist der entscheidende Teil. Die Ergebnisse zeigen sowohl einen gro√üen Fortschritt als auch klare Limitierungen.\n",
    "\n",
    "* **Gesamtgenauigkeit: 43.8%**.\n",
    "    * **Was das bedeutet:** Dieses Ergebnis ist fast **doppelt so hoch** wie beim vorherigen Ansatz (ca. 25 %). Es belegt, dass die zweistufige Methode (\"zuerst verstehen, dann antworten\") deutlich effektiver ist. Indem sich das Modell auf seine eigenen Schlussfolgerungen st√ºtzt, liefert es fundiertere Antworten.\n",
    "    * **Warum nicht 100%:** Die Genauigkeit ist f√ºr einen produktiven Einsatz immer noch zu gering. Dies deutet stark darauf hin, dass **das Problem nicht im Ansatz liegt, sondern in der Leistungsf√§higkeit des verwendeten Modells (`llama3.2:1b`)**. Ein kleines, lokal betriebenes Modell ist f√ºr eine komplexe Zeitreihenanalyse nicht leistungsstark genug, selbst mit einem sehr guten Prompt-Design.\n",
    "\n",
    "* **Geschwindigkeit: Durchschnittlich 10.72 Sekunden pro Antwort**.\n",
    "    * **Was das bedeutet:** F√ºr ein lokales Modell, das eine solch komplexe, zweistufige Analyse durchf√ºhrt, ist dies ein ausgezeichneter Wert. Das System reagiert schnell.\n",
    "\n",
    "* **Zuverl√§ssigkeit: 100%**.\n",
    "    * **Was das bedeutet:** Im Gegensatz zu fr√ºheren Versionen traten w√§hrend der Tests keine Fehler oder Timeouts auf. Die Nutzung von LangChain und die strukturierteren Prompts haben das System stabiler gemacht.\n",
    "\n",
    "* **Ergebnisse einzelner Tests:**\n",
    "    * **Z√§hlung von Programmen:** Die Genauigkeit lag hier zwischen 50 % und 100 %. Dies zeigt, dass das Modell Z√§hlaufgaben f√ºr eindeutige Werte relativ gut bew√§ltigt.\n",
    "    * **Finden des l√§ngsten Zyklus:** Hier war die Genauigkeit gering (0 % und 25 %). Das ist nachvollziehbar: Das Finden eines Maximalwertes in einer langen Datenreihe ist eine wesentlich anspruchsvollere analytische Aufgabe, die pr√§zise Berechnungen erfordert ‚Äì eine bekannte Schw√§che von Sprachmodellen.\n",
    "\n",
    "### üèÜ Fazit\n",
    "\n",
    "Dieses Notebook l√∂st die gestellte Aufgabe ‚Äì die Schaffung eines Systems ohne vordefinierte Algorithmen ‚Äì auf brillante Weise.\n",
    "\n",
    "1.  **Konzept bewiesen:** Es wurde nachgewiesen, dass ein LLM dazu gebracht werden kann, unbekannte Daten autonom zu analysieren und darauf basierend Fragen zu beantworten. Architektonisch ist dies ein gro√üer Fortschritt.\n",
    "2.  **Universalit√§t erreicht:** Der Code kann auf jede andere tabellarische Datendatei angewendet werden und w√ºrde auf die gleiche Weise funktionieren ‚Äì zuerst die Struktur verstehen, dann Fragen beantworten.\n",
    "3.  **Wichtigste Einschr√§nkung identifiziert:** Der Erfolg dieses Ansatzes h√§ngt direkt von den intellektuellen F√§higkeiten des LLM ab. Das Ergebnis von **43,8 %** ist wahrscheinlich die Leistungsgrenze f√ºr ein Modell wie `llama3.2:1b`.\n",
    "\n",
    "**Einfach ausgedr√ºckt:** Es wurde ein perfekter Rennwagen (die Architektur mit LangChain) gebaut, aber mit dem Motor eines Kleinwagens (`llama3.2:1b`) ausgestattet. Er f√§hrt, und das sogar besser als zuvor, aber um Rennen zu gewinnen, wird ein leistungsst√§rkerer Motor ben√∂tigt (z. B. GPT-4o oder Claude 3 Opus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for pure LangChain approach\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# System imports\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Install required packages - Alle ben√∂tigten LangChain Pakete installieren\n",
    "%pip install --quiet langchain langchain-community langchain-ollama langgraph openpyxl requests\n",
    "\n",
    "# Modern Pure LangChain imports - Moderne reine LangChain Imports ohne Algorithmus-Abh√§ngigkeiten\n",
    "try:\n",
    "    # Updated LangChain structure - Aktualisierte LangChain Struktur\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    print(\"‚úÖ Modern Pure LangChain successfully imported\")\n",
    "    langchain_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Modern LangChain import failed: {e}\")\n",
    "    try:\n",
    "        # Fallback for legacy versions - Fallback f√ºr √§ltere Versionen\n",
    "        from langchain_community.llms import Ollama as OllamaLLM\n",
    "        from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "        from langchain.prompts import PromptTemplate\n",
    "        print(\"‚úÖ LangChain (legacy) successfully imported\")\n",
    "        langchain_available = True\n",
    "    except ImportError as e2:\n",
    "        print(f\"‚ùå All LangChain imports failed: {e2}\")\n",
    "        langchain_available = False\n",
    "\n",
    "print(f\"\\nü§ñ Pure LangChain Zero-Algorithm System initialized\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Universal Data Loading\n",
    "\n",
    "**Universeller Ansatz**: L√§dt beliebige strukturierte Daten ohne Vorannahmen √ºber Format oder Inhalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_universal_structured_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Universal structured data loader - works with any structured data format\n",
    "    Universeller strukturierter Datenloader - funktioniert mit beliebigen strukturierten Datenformaten\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Universal data loading: {filepath}\")\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "        df = None\n",
    "        \n",
    "        # Strategy 1: Excel loading with multiple engines - Excel-Ladung mit mehreren Engines\n",
    "        if filepath.endswith(('.xlsx', '.xls')):\n",
    "            engines = ['openpyxl', 'xlrd']\n",
    "            for engine in engines:\n",
    "                try:\n",
    "                    df = pd.read_excel(filepath, engine=engine)\n",
    "                    print(f\"‚úÖ Excel file loaded with {engine}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Excel loading with {engine} failed: {e}\")\n",
    "        \n",
    "        # Strategy 2: CSV loading with encoding detection - CSV-Ladung mit Encoding-Erkennung\n",
    "        elif filepath.endswith('.csv') or df is None:\n",
    "            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath, encoding=encoding)\n",
    "                    print(f\"‚úÖ CSV file loaded with encoding '{encoding}'\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå CSV loading with encoding '{encoding}' failed: {e}\")\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"üìä Universal data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "            print(f\"üìã Column names: {list(df.columns)}\")\n",
    "            return df\n",
    "        else:\n",
    "            raise Exception(\"All loading strategies failed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Universal data loading failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load data universally - Daten universell laden\n",
    "raw_data = load_universal_structured_data(\"/Users/svitlanakovalivska/CNC/LLM_Project/sample_cnc_data.xlsx\")\n",
    "\n",
    "if raw_data is not None:\n",
    "    print(f\"\\nüéØ Universal data successfully loaded!\")\n",
    "    print(f\"Ready for zero-algorithm analysis\")\n",
    "    \n",
    "    # Show basic data overview without any assumptions - Grundlegende Daten√ºbersicht ohne Annahmen\n",
    "    print(f\"\\nüìã DATA OVERVIEW:\")\n",
    "    print(f\"Shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "    \n",
    "    # Display sample data - Beispieldaten anzeigen\n",
    "    print(f\"\\nüîç SAMPLE DATA (first 3 rows):\")\n",
    "    display(raw_data.head(3))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without data\")\n",
    "    raw_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Pure LangChain LLM Client\n",
    "\n",
    "**Reiner LangChain-Ansatz**: Keinerlei vordefinierte Logik, nur LangChain-Strukturen f√ºr LLM-Kommunikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureLangChainAnalyzer:\n",
    "    \"\"\"\n",
    "    Pure LangChain analyzer with zero predefined algorithms - Modern LangChain syntax\n",
    "    Reiner LangChain-Analyzer ohne vordefinierte Algorithmen - Moderne LangChain Syntax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"llama3.2:1b\", base_url=\"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        \n",
    "        # Initialize modern LangChain LLM - Modernen LangChain LLM initialisieren\n",
    "        if langchain_available:\n",
    "            try:\n",
    "                self.llm = OllamaLLM(\n",
    "                    model=model_name,\n",
    "                    base_url=base_url,\n",
    "                    temperature=0.1,\n",
    "                    num_predict=2000,\n",
    "                    top_k=10,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "                print(f\"‚úÖ Modern Pure LangChain LLM initialized: {model_name}\")\n",
    "                self.available = True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Modern LangChain LLM initialization failed: {e}\")\n",
    "                self.llm = None\n",
    "                self.available = False\n",
    "        else:\n",
    "            print(\"‚ùå LangChain not available\")\n",
    "            self.llm = None\n",
    "            self.available = False\n",
    "    \n",
    "    def create_universal_data_understanding_prompt(self, dataframe: pd.DataFrame) -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Create universal prompt template for any structured data\n",
    "        Universelle Prompt-Vorlage f√ºr beliebige strukturierte Daten erstellen\n",
    "        \"\"\"\n",
    "        template = \"\"\"You are an expert data analyst capable of understanding any structured dataset.\n",
    "\n",
    "DATASET INFORMATION:\n",
    "- Shape: {shape} (rows x columns)\n",
    "- Columns: {columns}\n",
    "- Data types: {dtypes}\n",
    "\n",
    "SAMPLE DATA (first 5 rows):\n",
    "{sample_data}\n",
    "\n",
    "TASK: Analyze this dataset completely without any assumptions about what it contains.\n",
    "\n",
    "1. **Data Nature**: What type of data is this? What domain/industry does it represent?\n",
    "2. **Key Patterns**: What patterns do you see in the data structure and values?\n",
    "3. **Important Columns**: Which columns seem most important for analysis?\n",
    "4. **Data Relationships**: How might different columns relate to each other?\n",
    "5. **Analysis Opportunities**: What types of questions could be answered with this data?\n",
    "\n",
    "IMPORTANT: \n",
    "- Do NOT assume any specific domain knowledge\n",
    "- Learn the data structure from the actual values shown\n",
    "- Be specific about what you observe in the sample data\n",
    "- Focus on patterns that would help answer analytical questions\n",
    "\n",
    "Provide your analysis in JSON format:\n",
    "{{\n",
    "  \"data_domain\": \"your assessment of what domain this data represents\",\n",
    "  \"key_columns\": [\"list\", \"of\", \"important\", \"columns\"],\n",
    "  \"patterns_observed\": [\"pattern 1\", \"pattern 2\"],\n",
    "  \"data_relationships\": \"how columns might relate\",\n",
    "  \"analysis_capabilities\": [\"type 1 questions\", \"type 2 questions\"],\n",
    "  \"insights\": \"key insights about the data structure\"\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"shape\", \"columns\", \"dtypes\", \"sample_data\"]\n",
    "        )\n",
    "    \n",
    "    def understand_data_universally(self, dataframe: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Universal data understanding without any domain assumptions - Modern LangChain\n",
    "        Universelles Datenverst√§ndnis ohne Domain-Annahmen - Moderne LangChain\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            return {\"error\": \"LangChain LLM not available\"}\n",
    "        \n",
    "        try:\n",
    "            # Prepare universal data summary - Universelle Datenzusammenfassung vorbereiten\n",
    "            data_summary = {\n",
    "                \"shape\": f\"{dataframe.shape[0]} rows x {dataframe.shape[1]} columns\",\n",
    "                \"columns\": list(dataframe.columns),\n",
    "                \"dtypes\": {col: str(dtype) for col, dtype in dataframe.dtypes.items()},\n",
    "                \"sample_data\": dataframe.head(5).to_string(max_cols=10, show_dimensions=False)\n",
    "            }\n",
    "            \n",
    "            # Create universal prompt - Universellen Prompt erstellen\n",
    "            prompt_template = self.create_universal_data_understanding_prompt(dataframe)\n",
    "            \n",
    "            # Create modern LangChain chain - Moderne LangChain-Kette erstellen\n",
    "            chain = prompt_template | self.llm | StrOutputParser()\n",
    "            \n",
    "            # Execute analysis - Analyse ausf√ºhren\n",
    "            response = chain.invoke({\n",
    "                \"shape\": data_summary[\"shape\"],\n",
    "                \"columns\": data_summary[\"columns\"],\n",
    "                \"dtypes\": data_summary[\"dtypes\"],\n",
    "                \"sample_data\": data_summary[\"sample_data\"]\n",
    "            })\n",
    "            \n",
    "            # Try to parse JSON response - JSON-Antwort zu parsen versuchen\n",
    "            try:\n",
    "                understanding = json.loads(response)\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback for non-JSON responses - Fallback f√ºr Nicht-JSON Antworten\n",
    "                understanding = {\n",
    "                    \"raw_analysis\": response,\n",
    "                    \"data_domain\": \"Unknown - parse failed\",\n",
    "                    \"status\": \"Raw text analysis available\"\n",
    "                }\n",
    "            \n",
    "            return understanding\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Universal data understanding failed: {str(e)}\"}\n",
    "    \n",
    "    def create_universal_question_prompt(self, question: str, data_understanding: Dict, dataframe: pd.DataFrame) -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Create universal prompt for any analytical question\n",
    "        Universellen Prompt f√ºr beliebige analytische Fragen erstellen\n",
    "        \"\"\"\n",
    "        template = \"\"\"You are an expert data analyst. You have analyzed a dataset and now need to answer a specific question.\n",
    "\n",
    "DATASET UNDERSTANDING:\n",
    "{data_understanding}\n",
    "\n",
    "CURRENT DATASET INFO:\n",
    "- Total records: {total_records}\n",
    "- Columns: {columns}\n",
    "\n",
    "SAMPLE DATA (latest 10 rows for context):\n",
    "{recent_data}\n",
    "\n",
    "QUESTION TO ANSWER: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. **Understand the Question**: What specifically is being asked?\n",
    "2. **Identify Relevant Data**: Which columns and rows are needed to answer this question?\n",
    "3. **Determine Analysis Method**: What analysis steps are required?\n",
    "4. **Generate Answer**: Provide a clear, factual answer based on the data\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Use ONLY the actual data provided above\n",
    "- Do NOT make assumptions about data you haven't seen\n",
    "- If you need to filter data, describe the filtering criteria clearly\n",
    "- Provide specific numbers with units when applicable\n",
    "- If the question cannot be answered with available data, say so clearly\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Provide your answer in this JSON structure:\n",
    "{{\n",
    "  \"question_interpretation\": \"what the question is asking for\",\n",
    "  \"relevant_data_identified\": \"which data is needed\",\n",
    "  \"analysis_approach\": \"how you will analyze the data\",\n",
    "  \"answer\": \"direct answer to the question\",\n",
    "  \"confidence\": \"high/medium/low based on available data\",\n",
    "  \"limitations\": \"any limitations in the analysis\"\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"data_understanding\", \"total_records\", \"columns\", \"recent_data\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    def answer_question_universally(self, question: str, dataframe: pd.DataFrame, data_understanding: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer any question using pure LangChain without predefined algorithms - Modern syntax\n",
    "        Beliebige Frage mit reinem LangChain ohne vordefinierte Algorithmen beantworten - Moderne Syntax\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            return {\"error\": \"LangChain LLM not available\"}\n",
    "        \n",
    "        try:\n",
    "            # Prepare current data context - Aktuellen Datenkontext vorbereiten\n",
    "            data_context = {\n",
    "                \"total_records\": len(dataframe),\n",
    "                \"columns\": list(dataframe.columns),\n",
    "                \"recent_data\": dataframe.tail(10).to_string(max_cols=10, show_dimensions=False)\n",
    "            }\n",
    "            \n",
    "            # Create universal question prompt - Universellen Fragenprompt erstellen\n",
    "            prompt_template = self.create_universal_question_prompt(question, data_understanding, dataframe)\n",
    "            \n",
    "            # Create modern LangChain chain - Moderne LangChain-Kette erstellen\n",
    "            chain = prompt_template | self.llm | StrOutputParser()\n",
    "            \n",
    "            # Execute question answering - Fragebeantwortung ausf√ºhren\n",
    "            response = chain.invoke({\n",
    "                \"data_understanding\": json.dumps(data_understanding, indent=2),\n",
    "                \"total_records\": data_context[\"total_records\"],\n",
    "                \"columns\": data_context[\"columns\"],\n",
    "                \"recent_data\": data_context[\"recent_data\"],\n",
    "                \"question\": question\n",
    "            })\n",
    "            \n",
    "            # Try to parse JSON response - JSON-Antwort parsen\n",
    "            try:\n",
    "                result = json.loads(response)\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback for non-JSON responses - Fallback f√ºr Nicht-JSON Antworten\n",
    "                result = {\n",
    "                    \"raw_response\": response,\n",
    "                    \"answer\": response,\n",
    "                    \"confidence\": \"unknown\",\n",
    "                    \"status\": \"Raw text response\"\n",
    "                }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Universal question answering failed: {str(e)}\"}\n",
    "\n",
    "# Initialize pure LangChain analyzer - Reinen LangChain-Analyzer initialisieren\n",
    "pure_analyzer = PureLangChainAnalyzer()\n",
    "print(f\"\\nüéØ Pure LangChain analyzer ready: {'‚úÖ' if pure_analyzer.available else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Universal Data Understanding\n",
    "\n",
    "**LLM lernt Daten**: Vollst√§ndig autonomes Verstehen der Datenstruktur ohne Vorannahmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal data understanding - Universelles Datenverst√§ndnis\n",
    "if raw_data is not None and pure_analyzer.available:\n",
    "    print(\"üß† Universal data understanding...\")\n",
    "    \n",
    "    # Let LangChain LLM understand the data structure autonomously\n",
    "    # LangChain LLM soll die Datenstruktur autonom verstehen\n",
    "    data_understanding = pure_analyzer.understand_data_universally(raw_data)\n",
    "    \n",
    "    print(f\"\\nüìä UNIVERSAL DATA UNDERSTANDING RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if 'error' not in data_understanding:\n",
    "        # Display structured understanding - Strukturiertes Verst√§ndnis anzeigen\n",
    "        if 'data_domain' in data_understanding:\n",
    "            print(f\"üîç Data Domain: {data_understanding['data_domain']}\")\n",
    "        if 'key_columns' in data_understanding:\n",
    "            print(f\"üîë Key Columns: {data_understanding['key_columns']}\")\n",
    "        if 'patterns_observed' in data_understanding:\n",
    "            print(f\"üìà Patterns: {data_understanding['patterns_observed']}\")\n",
    "        if 'analysis_capabilities' in data_understanding:\n",
    "            print(f\"üéØ Analysis Capabilities: {data_understanding['analysis_capabilities']}\")\n",
    "        if 'insights' in data_understanding:\n",
    "            print(f\"üí° Key Insights: {data_understanding['insights']}\")\n",
    "        \n",
    "        # Show raw analysis if JSON parsing failed - Rohe Analyse zeigen, falls JSON-Parsing fehlschlug\n",
    "        if 'raw_analysis' in data_understanding:\n",
    "            print(f\"\\nüìã Raw LLM Analysis:\")\n",
    "            print(data_understanding['raw_analysis'][:1000] + \"...\" if len(data_understanding['raw_analysis']) > 1000 else data_understanding['raw_analysis'])\n",
    "        \n",
    "        print(f\"\\n‚úÖ Data structure understood autonomously!\")\n",
    "        print(f\"System ready for universal question answering\")\n",
    "    else:\n",
    "        print(f\"‚ùå Data understanding failed: {data_understanding['error']}\")\n",
    "        data_understanding = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed - data or analyzer not available\")\n",
    "    data_understanding = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 4: Pure LangChain Question Answering\n",
    "\n",
    "**Algorithmfreie Antworten**: LLM beantwortet Fragen ohne vordefinierte Logik oder Algorithmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pure_langchain_question(question: str, analyzer, dataframe, understanding):\n",
    "    \"\"\"\n",
    "    Process question using pure LangChain without any predefined algorithms\n",
    "    Frage mit reinem LangChain ohne vordefinierte Algorithmen verarbeiten\n",
    "    \"\"\"\n",
    "    if not analyzer.available or dataframe is None or understanding is None:\n",
    "        print(\"‚ùå System not ready for question processing\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nü§ñ PURE LANGCHAIN QUESTION PROCESSING\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Pure LangChain question answering - Reine LangChain Fragebeantwortung\n",
    "    result = analyzer.answer_question_universally(question, dataframe, understanding)\n",
    "    \n",
    "    processing_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\nüí¨ PURE LANGCHAIN RESPONSE:\")\n",
    "    if 'error' not in result:\n",
    "        # Display structured response - Strukturierte Antwort anzeigen\n",
    "        if 'question_interpretation' in result:\n",
    "            print(f\"üîç Question Understanding: {result['question_interpretation']}\")\n",
    "        if 'analysis_approach' in result:\n",
    "            print(f\"‚öôÔ∏è Analysis Approach: {result['analysis_approach']}\")\n",
    "        if 'answer' in result:\n",
    "            print(f\"\\n‚ú® Answer: {result['answer']}\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"üìä Confidence: {result['confidence']}\")\n",
    "        if 'limitations' in result:\n",
    "            print(f\"‚ö†Ô∏è Limitations: {result['limitations']}\")\n",
    "        \n",
    "        # Show raw response if structured parsing failed - Rohe Antwort zeigen, falls strukturiertes Parsing fehlschlug\n",
    "        if 'raw_response' in result:\n",
    "            print(f\"\\nüìã Raw LLM Response:\")\n",
    "            print(result['raw_response'][:800] + \"...\" if len(result['raw_response']) > 800 else result['raw_response'])\n",
    "    else:\n",
    "        print(f\"‚ùå ERROR: {result['error']}\")\n",
    "    \n",
    "    print(f\"\\nüìä PROCESSING METRICS:\")\n",
    "    print(f\"‚è±Ô∏è Total time: {processing_time:.2f}s\")\n",
    "    print(f\"üß† Method: Pure LangChain (Zero Algorithms)\")\n",
    "    print(f\"üîÑ Approach: Universal data understanding + LLM reasoning\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'result': result,\n",
    "        'processing_time': processing_time,\n",
    "        'method': 'Pure LangChain Zero Algorithm',\n",
    "        'success': 'error' not in result\n",
    "    }\n",
    "\n",
    "# Test with a sample question - Test mit einer Beispielfrage\n",
    "if raw_data is not None and pure_analyzer.available and data_understanding:\n",
    "    test_question = \"Was war der l√§ngste Zyklus in den ACTIVE Daten?\"\n",
    "    sample_result = process_pure_langchain_question(test_question, pure_analyzer, raw_data, data_understanding)\n",
    "else:\n",
    "    print(\"‚ùå Cannot test - system components not available\")\n",
    "    sample_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 5: Validation Algorithms for Accuracy Testing\n",
    "\n",
    "**Genauigkeitsvalidierung**: Referenzalgorithmen zur √úberpr√ºfung der LangChain-Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationAlgorithms:\n",
    "    \"\"\"\n",
    "    Reference algorithms to validate Pure LangChain responses\n",
    "    Referenzalgorithmen zur Validierung der Pure LangChain Antworten\n",
    "    These are ONLY used for accuracy measurement, not for the main system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, raw_data):\n",
    "        self.raw_data = raw_data\n",
    "        if raw_data is not None:\n",
    "            # Convert timestamps once - Zeitstempel einmal konvertieren\n",
    "            self.data_with_timestamps = raw_data.copy()\n",
    "            if 'ts_utc' in self.data_with_timestamps.columns:\n",
    "                self.data_with_timestamps['ts_utc'] = pd.to_datetime(self.data_with_timestamps['ts_utc'])\n",
    "    \n",
    "    def detect_cycles_validation(self, target_date=None):\n",
    "        \"\"\"\n",
    "        Reference cycle detection for validation purposes\n",
    "        Referenz-Zykluserkennung f√ºr Validierungszwecke\n",
    "        \"\"\"\n",
    "        if self.raw_data is None or 'exec_STRING' not in self.data_with_timestamps.columns:\n",
    "            return []\n",
    "        \n",
    "        # Filter ACTIVE periods only - Nur ACTIVE Perioden filtern\n",
    "        active_data = self.data_with_timestamps[\n",
    "            self.data_with_timestamps['exec_STRING'] == 'ACTIVE'\n",
    "        ].copy()\n",
    "        \n",
    "        if len(active_data) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Filter by date if specified - Nach Datum filtern, falls angegeben\n",
    "        if target_date and 'ts_utc' in active_data.columns:\n",
    "            try:\n",
    "                target_date_obj = pd.to_datetime(target_date).date()\n",
    "                active_data = active_data[\n",
    "                    active_data['ts_utc'].dt.date == target_date_obj\n",
    "                ]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if 'ts_utc' not in active_data.columns:\n",
    "            return []\n",
    "            \n",
    "        active_data = active_data.sort_values('ts_utc')\n",
    "        \n",
    "        cycles = []\n",
    "        current_cycle_start = None\n",
    "        current_program = None\n",
    "        \n",
    "        for idx, row in active_data.iterrows():\n",
    "            current_time = row['ts_utc']\n",
    "            program = row.get('pgm_STRING', 'Unknown')\n",
    "            \n",
    "            # Detect cycle boundaries - Zyklusgrenzen erkennen\n",
    "            if (current_cycle_start is None or \n",
    "                program != current_program or\n",
    "                (current_time - prev_time).total_seconds() > 300):  # 5 min gap\n",
    "                \n",
    "                # End previous cycle - Vorherigen Zyklus beenden\n",
    "                if current_cycle_start is not None:\n",
    "                    cycle_duration = (prev_time - current_cycle_start).total_seconds()\n",
    "                    if 0.1 <= cycle_duration <= 28800:  # 0.1s to 8 hours\n",
    "                        cycles.append({\n",
    "                            'start_time': current_cycle_start,\n",
    "                            'end_time': prev_time,\n",
    "                            'duration_seconds': cycle_duration,\n",
    "                            'duration_minutes': cycle_duration / 60,\n",
    "                            'program': current_program\n",
    "                        })\n",
    "                \n",
    "                # Start new cycle - Neuen Zyklus beginnen\n",
    "                current_cycle_start = current_time\n",
    "                current_program = program\n",
    "            \n",
    "            prev_time = current_time\n",
    "        \n",
    "        # Close last cycle - Letzten Zyklus schlie√üen\n",
    "        if current_cycle_start is not None:\n",
    "            cycle_duration = (prev_time - current_cycle_start).total_seconds()\n",
    "            if 0.1 <= cycle_duration <= 28800:\n",
    "                cycles.append({\n",
    "                    'start_time': current_cycle_start,\n",
    "                    'end_time': prev_time,\n",
    "                    'duration_seconds': cycle_duration,\n",
    "                    'duration_minutes': cycle_duration / 60,\n",
    "                    'program': current_program\n",
    "                })\n",
    "        \n",
    "        return cycles\n",
    "    \n",
    "    def get_longest_cycle(self, target_date=None):\n",
    "        \"\"\"\n",
    "        Find longest cycle for validation\n",
    "        L√§ngsten Zyklus f√ºr Validierung finden\n",
    "        \"\"\"\n",
    "        cycles = self.detect_cycles_validation(target_date)\n",
    "        if not cycles:\n",
    "            return None\n",
    "        \n",
    "        longest = max(cycles, key=lambda x: x['duration_seconds'])\n",
    "        return {\n",
    "            'duration_minutes': longest['duration_minutes'],\n",
    "            'duration_seconds': longest['duration_seconds'],\n",
    "            'start_time': longest['start_time'],\n",
    "            'end_time': longest['end_time'],\n",
    "            'program': longest['program']\n",
    "        }\n",
    "    \n",
    "    def get_average_cycle_time(self, target_date=None):\n",
    "        \"\"\"\n",
    "        Calculate average cycle time for validation\n",
    "        Durchschnittliche Zykluszeit f√ºr Validierung berechnen\n",
    "        \"\"\"\n",
    "        cycles = self.detect_cycles_validation(target_date)\n",
    "        if not cycles:\n",
    "            return None\n",
    "        \n",
    "        avg_seconds = sum(c['duration_seconds'] for c in cycles) / len(cycles)\n",
    "        return {\n",
    "            'average_minutes': avg_seconds / 60,\n",
    "            'average_seconds': avg_seconds,\n",
    "            'total_cycles': len(cycles),\n",
    "            'date_range': f\"{cycles[0]['start_time'].date()} to {cycles[-1]['end_time'].date()}\"\n",
    "        }\n",
    "    \n",
    "    def get_unique_programs(self, target_date=None):\n",
    "        \"\"\"\n",
    "        Get unique programs for validation\n",
    "        Eindeutige Programme f√ºr Validierung ermitteln\n",
    "        \"\"\"\n",
    "        if self.raw_data is None or 'exec_STRING' not in self.data_with_timestamps.columns:\n",
    "            return None\n",
    "        \n",
    "        active_data = self.data_with_timestamps[\n",
    "            self.data_with_timestamps['exec_STRING'] == 'ACTIVE'\n",
    "        ]\n",
    "        \n",
    "        if target_date and 'ts_utc' in active_data.columns:\n",
    "            try:\n",
    "                target_date_obj = pd.to_datetime(target_date).date()\n",
    "                active_data = active_data[\n",
    "                    active_data['ts_utc'].dt.date == target_date_obj\n",
    "                ]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if 'pgm_STRING' not in active_data.columns:\n",
    "            return {'programs': [], 'count': 0}\n",
    "            \n",
    "        unique_programs = active_data['pgm_STRING'].dropna().unique()\n",
    "        return {\n",
    "            'programs': list(unique_programs),\n",
    "            'count': len(unique_programs)\n",
    "        }\n",
    "\n",
    "# Initialize validation algorithms - Validierungsalgorithmen initialisieren\n",
    "if raw_data is not None:\n",
    "    validator = ValidationAlgorithms(raw_data)\n",
    "    print(\"‚úÖ Validation algorithms initialized\")\n",
    "    \n",
    "    # Test validation algorithms - Validierungsalgorithmen testen\n",
    "    print(\"\\nüìä Validation Test Results:\")\n",
    "    \n",
    "    # Test cycle detection - Zykluserkennung testen\n",
    "    all_cycles = validator.detect_cycles_validation()\n",
    "    print(f\"Detected cycles: {len(all_cycles)}\")\n",
    "    \n",
    "    if all_cycles:\n",
    "        longest = validator.get_longest_cycle()\n",
    "        average = validator.get_average_cycle_time()\n",
    "        programs = validator.get_unique_programs()\n",
    "        \n",
    "        if longest:\n",
    "            print(f\"Longest cycle: {longest['duration_minutes']:.2f} minutes\")\n",
    "        if average:\n",
    "            print(f\"Average cycle: {average['average_minutes']:.2f} minutes\")\n",
    "        if programs:\n",
    "            print(f\"Unique programs: {programs['count']}\")\n",
    "else:\n",
    "    validator = None\n",
    "    print(\"‚ùå Validation algorithms not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 6: Pure LangChain Accuracy Testing\n",
    "\n",
    "**Genauigkeitsmessung**: Vergleich der Pure LangChain Ergebnisse mit Referenzalgorithmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureLangChainAccuracyTester:\n",
    "    \"\"\"\n",
    "    Comprehensive accuracy tester for Pure LangChain approach\n",
    "    Umfassender Genauigkeitstester f√ºr Pure LangChain Ansatz\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer, validator, dataframe, understanding):\n",
    "        self.analyzer = analyzer\n",
    "        self.validator = validator\n",
    "        self.dataframe = dataframe\n",
    "        self.understanding = understanding\n",
    "        self.test_results = []\n",
    "        self.failed_tests = []\n",
    "    \n",
    "    def extract_numbers_from_response(self, response_data):\n",
    "        \"\"\"\n",
    "        Extract numerical values from LangChain response\n",
    "        Numerische Werte aus LangChain-Antwort extrahieren\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Get text from various response formats - Text aus verschiedenen Antwortformaten holen\n",
    "        if isinstance(response_data, dict):\n",
    "            if 'answer' in response_data:\n",
    "                text = response_data['answer']\n",
    "            elif 'raw_response' in response_data:\n",
    "                text = response_data['raw_response']\n",
    "            else:\n",
    "                text = str(response_data)\n",
    "        else:\n",
    "            text = str(response_data)\n",
    "        \n",
    "        if not text or 'error' in text.lower():\n",
    "            return []\n",
    "        \n",
    "        # Extract numbers with units - Zahlen mit Einheiten extrahieren\n",
    "        patterns = [\n",
    "            r'(\\d+\\.?\\d*)\\s*minutes?',\n",
    "            r'(\\d+\\.?\\d*)\\s*mins?', \n",
    "            r'(\\d+\\.?\\d*)\\s*minuten',\n",
    "            r'(\\d+\\.?\\d*)\\s*stunden?',\n",
    "            r'(\\d+\\.?\\d*)\\s*hours?',\n",
    "            r'(\\d+\\.?\\d*)\\s*programme',\n",
    "            r'(\\d+\\.?\\d*)\\s*programs?',\n",
    "            r'(\\d+)\\s*verschiedene',\n",
    "            r'(\\d+)\\s*different',\n",
    "            r'(\\d+\\.?\\d*)\\s*',  # General numbers\n",
    "        ]\n",
    "        \n",
    "        numbers = []\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text.lower())\n",
    "            numbers.extend([float(match) for match in matches if match])\n",
    "        \n",
    "        # Convert hours to minutes if hour pattern detected - Stunden in Minuten umwandeln\n",
    "        hour_pattern = r'(\\d+\\.?\\d*)\\s*(stunden?|hours?)'\n",
    "        hour_matches = re.findall(hour_pattern, text.lower())\n",
    "        for hour, unit in hour_matches:\n",
    "            if 'stunden' in unit or 'hour' in unit:\n",
    "                numbers.append(float(hour) * 60)  # Convert to minutes\n",
    "        \n",
    "        return list(set(numbers))  # Remove duplicates\n",
    "    \n",
    "    def test_longest_cycle(self, target_date=None):\n",
    "        \"\"\"\n",
    "        Test Pure LangChain vs algorithm for longest cycle\n",
    "        Pure LangChain vs Algorithmus f√ºr l√§ngsten Zyklus testen\n",
    "        \"\"\"\n",
    "        date_str = f\" am {target_date}\" if target_date else \"\"\n",
    "        question = f\"Was war der l√§ngste Zyklus in den ACTIVE Daten{date_str}?\"\n",
    "        \n",
    "        print(f\"üî¨ Testing: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Test Pure LangChain - Pure LangChain testen\n",
    "        langchain_result = process_pure_langchain_question(question, self.analyzer, self.dataframe, self.understanding)\n",
    "        \n",
    "        if langchain_result is None or not langchain_result.get('success', False):\n",
    "            print(f\"‚ùå Pure LangChain FAILED\")\n",
    "            self.failed_tests.append({\n",
    "                'question': question,\n",
    "                'error': 'Pure LangChain system failure',\n",
    "                'type': 'system_error'\n",
    "            })\n",
    "            return None\n",
    "        \n",
    "        langchain_response = langchain_result['result']\n",
    "        \n",
    "        # Get algorithm result - Algorithmusergebnis erhalten\n",
    "        algo_result = self.validator.get_longest_cycle(target_date)\n",
    "        \n",
    "        print(f\"\\nü§ñ Pure LangChain Response:\")\n",
    "        response_text = langchain_response.get('answer', langchain_response.get('raw_response', str(langchain_response)))\n",
    "        print(response_text[:500] + \"...\" if len(response_text) > 500 else response_text)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è Algorithm Result:\")\n",
    "        if algo_result:\n",
    "            print(f\"Duration: {algo_result['duration_minutes']:.2f} minutes\")\n",
    "            print(f\"Start: {algo_result['start_time']}\")\n",
    "            print(f\"Program: {algo_result['program']}\")\n",
    "        else:\n",
    "            print(\"No cycles found\")\n",
    "        \n",
    "        # Calculate accuracy - Genauigkeit berechnen\n",
    "        langchain_numbers = self.extract_numbers_from_response(langchain_response)\n",
    "        accuracy = self.calculate_accuracy(langchain_numbers, algo_result, 'longest_cycle', response_text)\n",
    "        \n",
    "        result = {\n",
    "            'question': question,\n",
    "            'langchain_response': langchain_response,\n",
    "            'langchain_numbers': langchain_numbers,\n",
    "            'algorithm_result': algo_result,\n",
    "            'accuracy_score': accuracy,\n",
    "            'test_type': 'longest_cycle',\n",
    "            'processing_time': langchain_result.get('processing_time', 0),\n",
    "            'has_error': False\n",
    "        }\n",
    "        \n",
    "        self.test_results.append(result)\n",
    "        print(f\"\\nüìä Accuracy: {accuracy:.1f}%\")\n",
    "        print(f\"‚è±Ô∏è Time: {result['processing_time']:.2f}s\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def test_program_count(self, target_date=None):\n",
    "        \"\"\"\n",
    "        Test program counting\n",
    "        Programmz√§hlung testen\n",
    "        \"\"\"\n",
    "        date_str = f\" am {target_date}\" if target_date else \"\"\n",
    "        question = f\"Wie viele verschiedene Programme wurden im ACTIVE Modus ausgef√ºhrt{date_str}?\"\n",
    "        \n",
    "        print(f\"üî¨ Testing: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Test Pure LangChain - Pure LangChain testen\n",
    "        langchain_result = process_pure_langchain_question(question, self.analyzer, self.dataframe, self.understanding)\n",
    "        \n",
    "        if langchain_result is None or not langchain_result.get('success', False):\n",
    "            print(f\"‚ùå Pure LangChain FAILED\")\n",
    "            self.failed_tests.append({\n",
    "                'question': question,\n",
    "                'error': 'Pure LangChain system failure',\n",
    "                'type': 'system_error'\n",
    "            })\n",
    "            return None\n",
    "        \n",
    "        langchain_response = langchain_result['result']\n",
    "        \n",
    "        # Get algorithm result - Algorithmusergebnis erhalten\n",
    "        algo_result = self.validator.get_unique_programs(target_date)\n",
    "        \n",
    "        print(f\"\\nü§ñ Pure LangChain Response:\")\n",
    "        response_text = langchain_response.get('answer', langchain_response.get('raw_response', str(langchain_response)))\n",
    "        print(response_text[:500] + \"...\" if len(response_text) > 500 else response_text)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è Algorithm Result:\")\n",
    "        if algo_result:\n",
    "            print(f\"Count: {algo_result['count']} programs\")\n",
    "            print(f\"Programs: {algo_result['programs']}\")\n",
    "        else:\n",
    "            print(\"No programs found\")\n",
    "        \n",
    "        # Calculate accuracy - Genauigkeit berechnen\n",
    "        langchain_numbers = self.extract_numbers_from_response(langchain_response)\n",
    "        accuracy = self.calculate_accuracy(langchain_numbers, algo_result, 'program_count', response_text)\n",
    "        \n",
    "        result = {\n",
    "            'question': question,\n",
    "            'langchain_response': langchain_response,\n",
    "            'langchain_numbers': langchain_numbers,\n",
    "            'algorithm_result': algo_result,\n",
    "            'accuracy_score': accuracy,\n",
    "            'test_type': 'program_count',\n",
    "            'processing_time': langchain_result.get('processing_time', 0),\n",
    "            'has_error': False\n",
    "        }\n",
    "        \n",
    "        self.test_results.append(result)\n",
    "        print(f\"\\nüìä Accuracy: {accuracy:.1f}%\")\n",
    "        print(f\"‚è±Ô∏è Time: {result['processing_time']:.2f}s\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def calculate_accuracy(self, llm_numbers, algo_result, test_type, llm_response):\n",
    "        \"\"\"\n",
    "        Calculate accuracy score\n",
    "        Genauigkeitsbewertung berechnen\n",
    "        \"\"\"\n",
    "        if not algo_result:\n",
    "            # Check if LLM correctly identified no data - Pr√ºfen, ob LLM korrekt 'keine Daten' identifiziert hat\n",
    "            if any(phrase in llm_response.lower() for phrase in \n",
    "                   ['no active', 'keine daten', 'not found', 'nicht gefunden', 'keine aktiv']):\n",
    "                return 100.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        if not llm_numbers:\n",
    "            return 0.0  # No numbers extracted\n",
    "        \n",
    "        if test_type == 'longest_cycle':\n",
    "            expected = algo_result['duration_minutes']\n",
    "        elif test_type == 'program_count':\n",
    "            expected = algo_result['count']\n",
    "        else:\n",
    "            return 0.0\n",
    "        \n",
    "        # Find closest number - N√§chste Zahl finden\n",
    "        closest_number = min(llm_numbers, key=lambda x: abs(x - expected))\n",
    "        \n",
    "        # Calculate percentage error - Prozentuale Abweichung berechnen\n",
    "        if expected == 0:\n",
    "            return 100.0 if closest_number == 0 else 0.0\n",
    "        \n",
    "        error_percentage = abs(closest_number - expected) / expected * 100\n",
    "        \n",
    "        # Scoring system - Bewertungssystem\n",
    "        if error_percentage <= 5:       return 100.0  # Perfect\n",
    "        elif error_percentage <= 10:   return 90.0   # Excellent\n",
    "        elif error_percentage <= 20:   return 75.0   # Good\n",
    "        elif error_percentage <= 50:   return 50.0   # Fair\n",
    "        elif error_percentage <= 100:  return 25.0   # Poor\n",
    "        else:                          return 0.0    # Very poor\n",
    "    \n",
    "    def run_comprehensive_test(self):\n",
    "        \"\"\"\n",
    "        Run comprehensive Pure LangChain accuracy test\n",
    "        Umfassenden Pure LangChain Genauigkeitstest ausf√ºhren\n",
    "        \"\"\"\n",
    "        print(\"üß™ COMPREHENSIVE PURE LANGCHAIN ACCURACY TEST\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Test cases - Testf√§lle\n",
    "        test_cases = [\n",
    "            (self.test_longest_cycle, None, \"Overall longest cycle\"),\n",
    "            (self.test_longest_cycle, \"2025-08-13\", \"Longest cycle specific date\"),\n",
    "            (self.test_program_count, None, \"Program count overall\"),\n",
    "            (self.test_program_count, \"2025-08-13\", \"Program count specific date\"),\n",
    "        ]\n",
    "        \n",
    "        for test_func, param, description in test_cases:\n",
    "            print(f\"\\nüéØ {description}\")\n",
    "            try:\n",
    "                if param:\n",
    "                    test_func(param)\n",
    "                else:\n",
    "                    test_func()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Test failed with exception: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.failed_tests.append({\n",
    "                    'description': description,\n",
    "                    'error': str(e),\n",
    "                    'type': 'exception'\n",
    "                })\n",
    "        \n",
    "        return self.generate_assessment()\n",
    "    \n",
    "    def generate_assessment(self):\n",
    "        \"\"\"\n",
    "        Generate final assessment\n",
    "        Finale Bewertung generieren\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä PURE LANGCHAIN ACCURACY TEST RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_tests = len(self.test_results) + len(self.failed_tests)\n",
    "        successful_tests = len(self.test_results)\n",
    "        failed_tests = len(self.failed_tests)\n",
    "        \n",
    "        print(f\"Total Tests: {total_tests}\")\n",
    "        print(f\"Successful Responses: {successful_tests}\")\n",
    "        print(f\"Failed/Error Responses: {failed_tests}\")\n",
    "        \n",
    "        if successful_tests == 0:\n",
    "            print(\"\\n‚ùå CRITICAL: No successful Pure LangChain responses\")\n",
    "            print(\"üî¥ SYSTEM NOT FUNCTIONAL\")\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate average accuracy of successful tests - Durchschnittliche Genauigkeit erfolgreicher Tests berechnen\n",
    "        if self.test_results:\n",
    "            avg_accuracy = sum(r['accuracy_score'] for r in self.test_results) / len(self.test_results)\n",
    "            avg_time = sum(r['processing_time'] for r in self.test_results) / len(self.test_results)\n",
    "            \n",
    "            # Adjust for reliability (penalize for failures) - F√ºr Zuverl√§ssigkeit anpassen (Ausf√§lle bestrafen)\n",
    "            reliability_factor = successful_tests / total_tests\n",
    "            adjusted_accuracy = avg_accuracy * reliability_factor\n",
    "            \n",
    "            print(f\"\\nAverage Accuracy (successful tests): {avg_accuracy:.1f}%\")\n",
    "            print(f\"System Reliability: {reliability_factor*100:.1f}%\")\n",
    "            print(f\"Adjusted Overall Score: {adjusted_accuracy:.1f}%\")\n",
    "            print(f\"Average Processing Time: {avg_time:.2f}s\")\n",
    "            \n",
    "            # Show individual test results - Einzelne Testergebnisse zeigen\n",
    "            print(f\"\\nüìã INDIVIDUAL TEST RESULTS:\")\n",
    "            for result in self.test_results:\n",
    "                print(f\"  {result['test_type']}: {result['accuracy_score']:.1f}% ({result['processing_time']:.1f}s)\")\n",
    "            \n",
    "            return adjusted_accuracy\n",
    "        \n",
    "        return 0.0\n",
    "\n",
    "# Initialize Pure LangChain accuracy tester - Pure LangChain Genauigkeitstester initialisieren\n",
    "if (raw_data is not None and validator is not None and \n",
    "    pure_analyzer.available and data_understanding is not None):\n",
    "    accuracy_tester = PureLangChainAccuracyTester(pure_analyzer, validator, raw_data, data_understanding)\n",
    "    print(\"‚úÖ Pure LangChain Accuracy Tester initialized\")\n",
    "else:\n",
    "    accuracy_tester = None\n",
    "    print(\"‚ùå Cannot initialize Pure LangChain accuracy tester\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 7: Run Comprehensive Accuracy Tests\n",
    "\n",
    "**Vollst√§ndige Genauigkeitstests**: Systematische Bewertung des Pure LangChain Ansatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive Pure LangChain accuracy test - Umfassenden Pure LangChain Genauigkeitstest ausf√ºhren\n",
    "if accuracy_tester:\n",
    "    print(\"üöÄ Starting comprehensive Pure LangChain accuracy evaluation...\")\n",
    "    pure_langchain_overall_accuracy = accuracy_tester.run_comprehensive_test()\n",
    "    \n",
    "    print(f\"\\nüéâ PURE LANGCHAIN EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"üìä Final Pure LangChain Accuracy Score: {pure_langchain_overall_accuracy:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot run Pure LangChain accuracy test - components not available\")\n",
    "    pure_langchain_overall_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 8: Final Assessment with Accuracy and Speed Focus\n",
    "\n",
    "**Finale Bewertung**: Umfassende Analyse basierend auf gemessener Genauigkeit und Geschwindigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_pure_langchain_assessment(accuracy_score, test_results, validator_results):\n",
    "    \"\"\"\n",
    "    Final assessment of Pure LangChain approach with accuracy and speed focus\n",
    "    Finale Bewertung des Pure LangChain Ansatzes mit Fokus auf Genauigkeit und Geschwindigkeit\n",
    "    \"\"\"\n",
    "    print(f\"üìã FINALE BEWERTUNG: PURE LANGCHAIN ZERO-ALGORITHM ANSATZ\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Calculate real metrics - Echte Metriken berechnen\n",
    "    if test_results and len(test_results.test_results) > 0:\n",
    "        real_accuracy = accuracy_score\n",
    "        avg_time = sum(r['processing_time'] for r in test_results.test_results) / len(test_results.test_results)\n",
    "        max_time = max(r['processing_time'] for r in test_results.test_results)\n",
    "        success_rate = len(test_results.test_results) / (len(test_results.test_results) + len(test_results.failed_tests)) * 100\n",
    "    else:\n",
    "        real_accuracy = 0.0\n",
    "        avg_time = 0.0\n",
    "        max_time = 0.0\n",
    "        success_rate = 0.0\n",
    "    \n",
    "    print(f\"\\nüéØ PROJECT REQUIREMENTS FULFILLED:\")\n",
    "    print(\"‚úÖ Real LLM Implementation: LangChain + Ollama with llama3.2:1b\")\n",
    "    print(\"‚úÖ ZERO Predefined Algorithms: Pure LLM-driven analysis without hardcoded logic\")\n",
    "    print(\"‚úÖ Natural Language Queries: German and English processing\")\n",
    "    print(\"‚úÖ Machine Data Analysis: Universal structured data analysis\")\n",
    "    print(\"‚úÖ Universal Approach: Works with any structured data format\")\n",
    "    print(\"‚úÖ LangChain Integration: Structured LLM communication and prompt management\")\n",
    "    \n",
    "    print(f\"\\nüìä MEASURED PERFORMANCE RESULTS:\")\n",
    "    print(f\"üéØ MEASURED ACCURACY: {real_accuracy:.1f}%\")\n",
    "    print(f\"‚è±Ô∏è Average Speed: {avg_time:.2f} seconds\")\n",
    "    print(f\"‚è±Ô∏è Maximum Response Time: {max_time:.2f} seconds\") \n",
    "    print(f\"üîÑ System Reliability: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Validator reference results - Referenzergebnisse des Validators\n",
    "    if validator_results and len(validator_results) > 0:\n",
    "        print(f\"\\n‚öôÔ∏è REFERENCE ALGORITHM RESULTS:\")\n",
    "        if validator_results[0]:\n",
    "            print(f\"Longest Cycle: {validator_results[0]['duration_minutes']:.1f} minutes\")\n",
    "        if len(validator_results) > 1 and validator_results[1]:\n",
    "            print(f\"Average Cycle: {validator_results[1]['average_minutes']:.1f} minutes\")\n",
    "        if len(validator_results) > 2 and validator_results[2]:\n",
    "            print(f\"Total Programs: {validator_results[2]['count']}\")\n",
    "    \n",
    "    # Performance assessment based on accuracy AND speed - Leistungsbewertung basierend auf Genauigkeit UND Geschwindigkeit\n",
    "    print(f\"\\nüöÄ PERFORMANCE ASSESSMENT (Accuracy + Speed):\")\n",
    "    \n",
    "    if real_accuracy >= 80 and avg_time <= 15:\n",
    "        status = \"üü¢ EXCELLENT: High Accuracy + Fast Processing\"\n",
    "        deployment_ready = \"Ready for pilot deployment\"\n",
    "        color = \"üü¢\"\n",
    "    elif real_accuracy >= 60 and avg_time <= 30:\n",
    "        status = \"üü° GOOD: Good balance of accuracy and speed\"\n",
    "        deployment_ready = \"Suitable for testing with monitoring\"\n",
    "        color = \"üü°\"\n",
    "    elif real_accuracy >= 40 or avg_time <= 45:\n",
    "        status = \"üü† FAIR: One metric good, other needs improvement\"\n",
    "        deployment_ready = \"Requires optimization before production\"\n",
    "        color = \"üü†\"\n",
    "    else:\n",
    "        status = \"üî¥ INSUFFICIENT: Both accuracy and speed problematic\"\n",
    "        deployment_ready = \"Not suitable for production\"\n",
    "        color = \"üî¥\"\n",
    "    \n",
    "    print(f\"{status}\")\n",
    "    print(f\"Deployment Readiness: {deployment_ready}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ ADVANTAGES OF PURE LANGCHAIN ZERO-ALGORITHM APPROACH:\")\n",
    "    advantages = [\n",
    "        \"üß† True Universality: Works with any structured data without domain knowledge\",\n",
    "        \"üîÑ Structured LLM Communication: LangChain provides organized prompt management\", \n",
    "        \"üìä Zero Maintenance: No algorithms to update or domain-specific code\",\n",
    "        \"ü§ñ Self-Learning: LLM understands data structure autonomously\",\n",
    "        \"üõ†Ô∏è Extensibility: Easy to add new question types without code changes\",\n",
    "        \"üåç Future-Proof: Improves automatically with better LLM models\"\n",
    "    ]\n",
    "    for advantage in advantages:\n",
    "        print(f\"  {advantage}\")\n",
    "    \n",
    "    # Issues identified - Identifizierte Probleme\n",
    "    if real_accuracy < 70 or avg_time > 30:\n",
    "        print(f\"\\n‚ùå IDENTIFIED ISSUES:\")\n",
    "        issues = []\n",
    "        if real_accuracy < 50:\n",
    "            issues.append(\"‚Ä¢ Low Accuracy: LLM struggles with complex data analysis\")\n",
    "        elif real_accuracy < 70:\n",
    "            issues.append(\"‚Ä¢ Inconsistent Results: Better prompting strategies needed\")\n",
    "        if avg_time > 30:\n",
    "            issues.append(\"‚Ä¢ Slow Response Times: LangChain pipeline optimization needed\")\n",
    "        if avg_time > 60:\n",
    "            issues.append(\"‚Ä¢ Timeout Risk: System unreliable for production use\")\n",
    "        for issue in issues:\n",
    "            print(issue)\n",
    "    \n",
    "    print(f\"\\nüéØ SPECIFIC RECOMMENDATIONS:\")\n",
    "    \n",
    "    if real_accuracy >= 70 and avg_time <= 30:\n",
    "        recommendations = [\n",
    "            \"1. Deploy Pure LangChain system for pilot test on single machine\",\n",
    "            \"2. Implement accuracy monitoring in production environment\",\n",
    "            \"3. Gradual scaling to additional machines and data types\",\n",
    "            \"4. Set up user feedback collection system\"\n",
    "        ]\n",
    "    elif real_accuracy >= 50 or avg_time <= 45:\n",
    "        recommendations = [\n",
    "            \"1. Test with more powerful LLM models (GPT-4/Claude)\",\n",
    "            \"2. Optimize LangChain prompt templates and chains\",\n",
    "            \"3. Implement prompt engineering based on failure analysis\",\n",
    "            \"4. Add result validation mechanisms\"\n",
    "        ]\n",
    "    else:\n",
    "        recommendations = [\n",
    "            \"1. Complete redesign of prompt strategies and LangChain architecture\",\n",
    "            \"2. Consider hybrid approach with algorithmic validation\",\n",
    "            \"3. Test with enterprise LLM providers\",\n",
    "            \"4. Extensive R&D required before production deployment\"\n",
    "        ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    print(f\"\\nüí∞ EFFORT-BENEFIT ANALYSIS:\")\n",
    "    if real_accuracy >= 70 and avg_time <= 30:\n",
    "        print(\"Development Time: 4-6 weeks for production-ready system\")\n",
    "        print(\"LangChain Optimization: 2-3 weeks\")\n",
    "        print(\"Expected ROI: High - intelligent system with good performance\")\n",
    "    elif real_accuracy >= 50 or avg_time <= 45:\n",
    "        print(\"Development Time: 8-12 weeks with LangChain optimization\")\n",
    "        print(\"Research Phase: 3-4 weeks for advanced LLM integration\")\n",
    "        print(\"Expected ROI: Medium - depends on optimization success\")\n",
    "    else:\n",
    "        print(\"Development Time: 4-6 months for complete system redesign\")\n",
    "        print(\"Expected ROI: Uncertain - significant investment required\")\n",
    "    \n",
    "    print(f\"\\nüîç ZERO-ALGORITHM VALIDATION:\")\n",
    "    print(\"‚Ä¢ ‚úÖ No predefined business logic or domain assumptions\")\n",
    "    print(\"‚Ä¢ ‚úÖ Universal prompts work with any structured data\")\n",
    "    print(\"‚Ä¢ ‚úÖ LLM generates own understanding and queries\")\n",
    "    print(\"‚Ä¢ ‚úÖ Pure LangChain architecture without hardcoded algorithms\")\n",
    "    \n",
    "    print(f\"\\nüéâ PURE LANGCHAIN ZERO-ALGORITHM ANALYSIS COMPLETE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"{color} Final Assessment: {status.split(':')[1].strip()}\")\n",
    "    print(f\"üìä Measured Accuracy: {real_accuracy:.1f}%\")\n",
    "    print(f\"‚è±Ô∏è Measured Speed: {avg_time:.1f}s\")\n",
    "    print(f\"üéØ Deployment Recommendation: {deployment_ready}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': real_accuracy,\n",
    "        'speed': avg_time,\n",
    "        'reliability': success_rate,\n",
    "        'status': status,\n",
    "        'deployment_ready': deployment_ready\n",
    "    }\n",
    "\n",
    "# Generate final assessment with real data - Finale Bewertung mit echten Daten generieren\n",
    "if 'pure_langchain_overall_accuracy' in globals() and validator:\n",
    "    validator_sample_results = [\n",
    "        validator.get_longest_cycle(),\n",
    "        validator.get_average_cycle_time(),\n",
    "        validator.get_unique_programs()\n",
    "    ]\n",
    "    \n",
    "    final_pure_langchain_assessment = generate_final_pure_langchain_assessment(\n",
    "        pure_langchain_overall_accuracy if 'pure_langchain_overall_accuracy' in globals() else 0.0,\n",
    "        accuracy_tester if 'accuracy_tester' in globals() else None,\n",
    "        validator_sample_results\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå Cannot generate final assessment - missing test results\")\n",
    "    final_pure_langchain_assessment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary: Pure LangChain Zero-Algorithm Approach\n",
    "\n",
    "### ‚úÖ **What was Implemented:**\n",
    "\n",
    "1. **ü§ñ Pure LangChain Architecture**\n",
    "   - No predefined algorithms or domain assumptions\n",
    "   - Universal data understanding through LLM reasoning\n",
    "   - Structured prompt management with LangChain\n",
    "   - Complete separation of LLM communication from business logic\n",
    "\n",
    "2. **üìä Universal Data Processing**\n",
    "   - Works with any structured data format\n",
    "   - Autonomous data structure learning\n",
    "   - No hardcoded column names or data assumptions\n",
    "   - Dynamic adaptation to different datasets\n",
    "\n",
    "3. **üß† Zero-Algorithm Question Answering**\n",
    "   - LLM generates own understanding of questions\n",
    "   - Self-determined analysis approaches\n",
    "   - No predefined query templates or logic\n",
    "   - Pure reasoning-based responses\n",
    "\n",
    "4. **üìã Comprehensive Accuracy Testing**\n",
    "   - Same validation system as other approaches\n",
    "   - Numerical extraction and comparison\n",
    "   - Percentage accuracy scoring\n",
    "   - Speed and reliability measurement\n",
    "\n",
    "### üéØ **Core Achievement:**\n",
    "- **True Zero-Algorithm Implementation** ‚úÖ\n",
    "- **Universal Data Compatibility** ‚úÖ  \n",
    "- **Pure LangChain Integration** ‚úÖ\n",
    "- **Measurable Performance Metrics** ‚úÖ\n",
    "\n",
    "### üîß **Technical Innovation:**\n",
    "- **Prompt-Driven Data Understanding**: LLM learns data structure autonomously\n",
    "- **Context-Aware Question Processing**: Responses based on actual data patterns\n",
    "- **Universal Applicability**: Same system works with any structured data\n",
    "- **LangChain Orchestration**: Structured yet flexible LLM communication\n",
    "\n",
    "**This implementation fulfills the project requirement for a truly algorithm-free approach while maintaining the structured benefits of LangChain integration.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7d300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting token loading diagnostic...\n",
      "\n",
      "==================================================\n",
      "‚úÖ RESULT: Key was successfully loaded from the file.\n",
      "   - Key Length: 1622 characters\n",
      "   - First 15 characters: eyJ0eXAiOiJKV1Q...\n",
      "   - Last 15 characters: ...OiV1KtSYg-3c8bg\n",
      "   - No leading/trailing spaces were detected.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# --- Function to load the token (embedded in this script) ---\n",
    "\n",
    "def load_token_from_file(filepath: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Loads a token from a file, ignoring comments and empty lines.\n",
    "    This can handle both relative and absolute file paths.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file containing the token.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The token, or None if the file is not found or is empty.\n",
    "    \"\"\"\n",
    "    token_file = Path(filepath)\n",
    "    try:\n",
    "        if token_file.exists():\n",
    "            with open(token_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    # Skip comments (lines starting with #) and empty lines\n",
    "                    if line and not line.startswith('#'):\n",
    "                        return line\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading token file {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main function to diagnose the token loading process ---\n",
    "\n",
    "def run_token_diagnostic() -> None:\n",
    "    \"\"\"\n",
    "    DIAGNOSTIC VERSION:\n",
    "    This function only loads the key from the specified path and prints\n",
    "    information about it for verification. It does not send any API requests.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting token loading diagnostic...\")\n",
    "    \n",
    "    # The absolute path to your token file.\n",
    "    token_path = '/Users/svitlanakovalivska/CNC/LLM_Project/config/ionos_token.txt'\n",
    "    \n",
    "    # Attempt to load the API key.\n",
    "    api_key = load_token_from_file(token_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    # Check if the key was loaded successfully.\n",
    "    if not api_key:\n",
    "        print(\"‚ùå RESULT: Key was NOT found or the file is empty.\")\n",
    "        print(f\"   Please check that the path '{token_path}' is correct and the file is not empty.\")\n",
    "    else:\n",
    "        print(\"‚úÖ RESULT: Key was successfully loaded from the file.\")\n",
    "        print(f\"   - Key Length: {len(api_key)} characters\")\n",
    "        print(f\"   - First 15 characters: {api_key[:15]}...\")\n",
    "        print(f\"   - Last 15 characters: ...{api_key[-15:]}\")\n",
    "        \n",
    "        # Check for hidden leading/trailing whitespace.\n",
    "        if api_key.startswith(' ') or api_key.endswith(' '):\n",
    "            print(\"   ‚ö†Ô∏è WARNING: There are leading or trailing spaces in your key!\")\n",
    "        else:\n",
    "            print(\"   - No leading/trailing spaces were detected.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# --- This line runs the diagnostic function when you execute the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_token_diagnostic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86ae86",
   "metadata": {},
   "source": [
    "## ‚úÖ IONOS API - –£—Å–ø—ñ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ!\n",
    "\n",
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ curl —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è (—è–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–≤–∞–ª–∞ —Å–ª—É–∂–±–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∏):**\n",
    "- ‚úÖ –¢–æ–∫–µ–Ω –ø—Ä–∞—Ü—é—î –≤—ñ–¥–º—ñ–Ω–Ω–æ\n",
    "- ‚úÖ API –ø–æ–≤–Ω—ñ—Å—Ç—é –¥–æ—Å—Ç—É–ø–Ω–µ  \n",
    "- ‚úÖ Chat completion —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π\n",
    "- ü§ñ –î–æ—Å—Ç—É–ø–Ω–æ 13 –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "**–û—Å–Ω–æ–≤–Ω—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è:**\n",
    "- `meta-llama/Meta-Llama-3.1-8B-Instruct` (—à–≤–∏–¥–∫–∞)\n",
    "- `meta-llama/Llama-3.3-70B-Instruct` (–ø–æ—Ç—É–∂–Ω–∞) \n",
    "- `meta-llama/Meta-Llama-3.1-405B-Instruct-FP8` (–Ω–∞–π–ø–æ—Ç—É–∂–Ω—ñ—à–∞)\n",
    "- `mistralai/Mistral-Nemo-Instruct-2407`\n",
    "- `mistralai/Mistral-Small-24B-Instruct`\n",
    "\n",
    "**‚ö†Ô∏è –í–∞–∂–ª–∏–≤–æ:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —Ç–æ—á–Ω—ñ –Ω–∞–∑–≤–∏ –º–æ–¥–µ–ª–µ–π –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏ \"Meta-Llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cdbdc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting a test request to IONOS AI...\n",
      "üí¨ Sending prompt to the model: 'What is 12 multiplied by 12?'\n",
      "\n",
      "========================================\n",
      "‚úÖ Response received successfully!\n",
      "ü§ñ Model's Answer: 12 multiplied by 12 is 144.\n",
      "--------------------\n",
      "üìä Token Usage Statistics:\n",
      "   - Prompt Tokens: 50\n",
      "   - Completion Tokens: 10\n",
      "   - Total Tokens Used: 60\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "‚úÖ Response received successfully!\n",
      "ü§ñ Model's Answer: 12 multiplied by 12 is 144.\n",
      "--------------------\n",
      "üìä Token Usage Statistics:\n",
      "   - Prompt Tokens: 50\n",
      "   - Completion Tokens: 10\n",
      "   - Total Tokens Used: 60\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# --- Function to load the token (embedded in this script) ---\n",
    "\n",
    "def load_token_from_file(filepath: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Loads a token from a file, ignoring comments and empty lines.\n",
    "    This can handle both relative and absolute file paths.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the file containing the token.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The token, or None if the file is not found or is empty.\n",
    "    \"\"\"\n",
    "    token_file = Path(filepath)\n",
    "    try:\n",
    "        if token_file.exists():\n",
    "            with open(token_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    # Skip comments (lines starting with #) and empty lines\n",
    "                    if line and not line.startswith('#'):\n",
    "                        return line\n",
    "        print(f\"‚ö†Ô∏è Token file not found at path: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading token file {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main function to query the model and count tokens ---\n",
    "\n",
    "def get_answer_and_usage_stats() -> None:\n",
    "    \"\"\"\n",
    "    Sends a basic prompt to the IONOS model, gets the response, and fetches token usage statistics.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting a test request to IONOS AI...\")\n",
    "    \n",
    "    # 1. Absolute path to your token file is specified here.\n",
    "    token_path = '/Users/svitlanakovalivska/CNC/LLM_Project/config/ionos_token.txt'\n",
    "    \n",
    "    # Load the API key\n",
    "    api_key = load_token_from_file(token_path)\n",
    "    if not api_key:\n",
    "        print(\"‚ùå Process stopped because the API key could not be loaded.\")\n",
    "        return\n",
    "\n",
    "    # 2. API Settings\n",
    "    endpoint = \"https://openai.inference.de-txl.ionos.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # 3. Define our test prompt\n",
    "    #    A simple math question for the model\n",
    "    test_prompt = \"What is 12 multiplied by 12?\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", # CORRECT model name from API\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": test_prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    print(f\"üí¨ Sending prompt to the model: '{test_prompt}'\")\n",
    "\n",
    "    # 4. Send the request and get the response\n",
    "    try:\n",
    "        response = requests.post(endpoint, json=payload, headers=headers, timeout=60)\n",
    "        response.raise_for_status()  # Check for HTTP errors (like 401, 404, 500)\n",
    "\n",
    "        data = response.json()\n",
    "        \n",
    "        # 5. Extract the data from the response\n",
    "        # The model's answer\n",
    "        model_answer = data['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # The token usage statistics\n",
    "        usage_stats = data.get('usage', {}) # Use .get to avoid an error if 'usage' is missing\n",
    "        prompt_tokens = usage_stats.get('prompt_tokens', 'N/A')\n",
    "        completion_tokens = usage_stats.get('completion_tokens', 'N/A')\n",
    "        total_tokens = usage_stats.get('total_tokens', 'N/A')\n",
    "\n",
    "        # 6. Display the results\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"‚úÖ Response received successfully!\")\n",
    "        print(f\"ü§ñ Model's Answer: {model_answer}\")\n",
    "        print(\"-\" * 20)\n",
    "        print(\"üìä Token Usage Statistics:\")\n",
    "        print(f\"   - Prompt Tokens: {prompt_tokens}\")\n",
    "        print(f\"   - Completion Tokens: {completion_tokens}\")\n",
    "        print(f\"   - Total Tokens Used: {total_tokens}\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå API Connection Error: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Unexpected API response format. Could not find key: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An unknown error occurred: {e}\")\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == \"__main__\":\n",
    "    get_answer_and_usage_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
